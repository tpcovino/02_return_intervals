---
author: "YOUR NAME HERE"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

# Learning module 2 - Return intervals (20 pts)

## Intro

In this lab we will look at some precipitation data to get an idea of return intervals for a given rain event. A return interval is the inverse of the probability. So if a certain rain even has a 10% probability of happening any year it has a 1/p return interval, so: R = 1/0.1 = 10 years. This means on average you can expect that size event about every ten years. From a probability perspective it is actually more correct to state that there is a 10% chance of that size rain event in any year. The reason this is better is that it communicates that you certainly can have a 10% probability event occur in back-to-back years.

After computing some return intervals we will then use some of the simpler rainfall-runoff modeling approaches (the rational method and the curve number method) to simulate runoff for a hypothetical basin in our next unit. 

This is knitr settings. Knitr is a package that will turn this Rmd to an html. 
```{r}
knitr::opts_chunk$set()
```

## Packages

We have a few new packages with this exercise. Those include GHCNr and leaflet, and a couple of spatial packages. GHNCr is a package used to download NOAA climate data, leaflet is a package for interactive mapping, and terra and sf are spatial packages that help us filter observation data by latitude and longitude.

Recall that if you have not installed these packages on your computer you will need to run:
install.packages("GHCNr") and so on for the others. 

```{r}



```

## Precipitation return intervals 

First, let's start by getting some precipitation (P) data. Access to NOAA data is through the web API
[Link](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily){target="_blank"} using the GHCNr package. The function stations() will download the site information (lat, lon, station no.) for all stations in the GHCND network.

```{r Prepare Data}
#1. See what data exists
# download all available station metadata
# THIS CAN TAKE A WHILE! I'd suggest to run it only once and then comment it out with #.
#station_data_download <-

# View(station_data_download) #uncomment to see how View() works, or click on the variable name in the Global Environment in the top-right panel. Even without opening the df, we can see that there are over 200,000 obs. (rows). 

# Wow, there are a lot of stations available. Let's go through a quick exercise to demonstrate how you can find a station if you know the area of interest but not the station number.

# 2. Subset spatially 
# Since this dataframe provides lat lon data, we can filter by latitude and longitude to find stations within a certain 'bounding box'. Geospatial packages in R like sf and tigris can make retrieving latitude and longitude info for counties and states easy. 

# a. Get US county boundaries
#tigris::counties()can fetch counties for a specific state.
#mt_counties <- 
# cb = TRUE gives a simpler (generalized) geometry

# b. Filter for a specific county (e.g., Gallatin county, home of Bozeman and MSU)
#gallatin <- 

# c. We can use the geometry information in this dataframe to generate a bounding box using plotly
# bbox <- 
#view(bbox)

# d. Great, now we can use min and max box values to spatially filter the list of stations. 
#stations_gal <- 

# 3. Explore metadata visually 
# Now, let's create a zoomable map with the stations around Gallatin County/Bozeman. Zoom into Bozeman and click around and find the station id for the precip gauge with the LONGEST timeseries. If you click on a station symbol, the first and last data year will appear. Obviously you can get this information from the stations_gal data frame, but I wanted to show you the mapping capabilities. 

#gauge_map <- leaflet() 
    # add a basemap

    # add markers for your station. The parameters are pretty self-explanatory
#  addAwesomeMarkers()

```  

For this exercise, we want station USC00241044, which runs from 1892 to now. 
```{r}
# 4. Pull a long time series. We can pull daily data directly from the url. This will store a file to your working directory that we will import and format in the next step. 



#download.file(url, destfile = paste0(station_id, ".dly"))
```

Now you should have a file named USC00241044.dly in the working directory. A .dly file is basically a “compact daily weather diary” from NOAA where each row records a month of a single weather variable (like temperature or precipitation) for a station, with one column per day. It’s designed for computers to read efficiently, but we usually turn it into a regular table so people can understand it easily. This next chunk might look like a lot, but try to understand what each line is doing. Lets check it out:

```{r}
# GHCN-Daily format: https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt

# Check out the .txt link above. This is tells us how the .dly file is formatted (part III). You will see that each column in the .dly file is set by a fixed number of spaces. The next line cells R how wide each column is in the .dly file: the first 11 characters are the station ID, etc. 


# Transform the imported data to a long dataframe like:
#str(long)
#tibble [176,918 × 7] (S3: tbl_df/tbl/data.frame)
# $ ID     : chr [1:176918] "USC00241044" "USC00241044" "USC00241044" "USC00241044" ...
# $ YEAR   : int [1:176918] 1892 1892 1892 1892 1892 1892 1892 1892 1892 1892 ...
# $ MONTH  : int [1:176918] 4 4 4 4 4 4 4 4 4 4 ...
# $ ELEMENT: chr [1:176918] "TMAX" "TMAX" "TMAX" "TMAX" ...
# $ DAY    : int [1:176918] 8 9 10 11 12 13 14 15 16 17 ...
# $ VALUE  : int [1:176918] 78 128 100 89 94 139 172 189 128 72 ...
# $ DATE   : Date[1:176918], format: "1892-04-08" 
```

Check out the 'long' dataframe. This should look similar to the plotted dataframes last week. From here, plotting is much cleaner than a wide dataframe, especially if we are interested in combining variables into a figure. One easy way to do this is to use facet_wrap()to generate individual plots for each element:

```{r}
#ggplot()
```

However, for the sake of demonstration, we'll work with tables that are easier to read. Pivot wider to make the dataframe easier for our 'human' inspection.

```{r}
climate_data <- long %>%
  select(DATE, ELEMENT, VALUE) %>%
  pivot_wider(names_from = ELEMENT, values_from = VALUE)

head(climate_data)
```

Now we have some climate data in a 'wide' format that we are familiar with. Take a minute to inspect the climate_data df. First, just looking at the df in table format, we see that the data don't actually start until 1894. It is also always a good idea to just plot some data. Below plot prcp, snow, tmax and tmin. You can just make 4 different plots. This is just for visual inspection. This part of the process is called exploratory data analysis (EDA). This should always be the first step when downloading data whether you download the data from the internet or from a sensor in the field. 

```{r}
# Explore the data (also check other ELEMENTs here in the exploratory process)
```

How many millimeters of precipitation on average does Bozeman receive? You might have noticed that the daily values seem to be much too large! Did you notice that? This is a great skill to develop. Have a look at the data and ask "are these numbers reasonable?". In this case, the answer would be no!

One thing to note is that NOAA data comes in tenths of degrees for temp and tenths of millimeters for precip. See variable definitions on the .txt link provided above. So, we need to clean up the df a bit. Let's do that here. 

```{r}
# Filter or transform as needed
climate_data_corr <- climate_data %>% 
  mutate(
    # division by 10 turns it into a normal decimal, 15.6 instead of 156
    TMIN = TMIN / 10,
    TMAX = TMAX / 10,
    PRCP = PRCP / 10,
    SNOW = SNOW / 10) 
```

Now that we've converted units, it is a good idea to plot your data again for some EDA. Make plots of each of the variables over time to inspect. 
```{r}
# Check filtering and/or transformation
# ggplotly(
#   climate_data_corr %>% 
#   ggplot(aes(x = date, y = prcp)) +
#     geom_point() +
#   geom_line(linetype = "dashed")
# ) # I commented this out just because plotly is slow. However, you should run once to see ggplotly interactive features.

climate_data_corr %>% 
  ggplot(aes(x = DATE, y = PRCP)) +
  geom_point()

```

How do the data look? Do they make sense? Do you see any strange values? 

There is a large snow event in 1951. We can assume that is "real", so let's keep it in the analysis. But you should think through how you could exclude it from the analysis. How could you use the filter function to do that? 

Next, we want to use some skills from the hydrograph sep lab to add a water year (WY) column. Try that here. 

```{r}
# Add calculations and summaries
climate_data_corr <- climate_data_corr %>% 
  mutate(month = month(DATE),
         year = year(DATE),
         wy = if_else(month > 9, year + 1, year))
```

I like to rearrange the order of columns using: 

```{r}
climate_data_corr <- climate_data_corr %>% 
  select(DATE, wy, everything())
```

Now, create a new df called climate_an where you calculate the total P (i.e., the sum) for each water year. Use group_by and summarize (or better yet, reframe). Also keep in mind that you will need to deal with NA values in the df. How do you do that in reframe or summarize? As a note, reframe can be used instead of summarize and is a more general purpose function. You can try each.   
```{r}
climate_an <- climate_data_corr %>% 
  group_by(wy) %>% 
  reframe(tot_p = sum(PRCP,  na.rm = TRUE), 
            mean_max = mean(TMAX, na.rm = TRUE), 
            mean_min = min(TMIN, na.rm = TRUE)) # I also calculate some temp stats here. Just out of curiousity. We don't use them in this lab. 
  
```

What happens if you don't deal with NA values by using something like na.rm = TRUE?

Now, plot total annual P on the Y and water year on the x. What do you see? 

```{r}
climate_an %>% 
  ggplot(aes(x = wy, y = tot_p)) +
  geom_point() +
  geom_line(linetype = "dashed", color = "blue")
```

Now let's calculate some probabilities. Look up the `pnorm()` function for this (either type it into the Help window, or type ?pnorm in the console. You only need x, the mean, and standard deviation (sd) for the calculations.

### **Q1. (3 pts) What is the probability that the annual precipitation in a given year is less than 400 mm? This is the F(A) in the CDF in the probability lecture slides.** 

```{r}
# Analysis and interpretation
p_400 <- pnorm(400, mean(climate_an$tot_p), sd(climate_an$tot_p))
p_400
```

Q1 ANSWER:

### **Q2. (3 pts) What is the probability that the maximum annual precipitation in a given year is GREATER than 500 mm?** 

```{r}
p_500 <- pnorm(500, mean(climate_an$tot_p), sd(climate_an$tot_p))
ex_500 <- 1 - p_500
ex_500
```

Q2 ANSWER:

### **Q3. (3 pts) What is the probability that the annual P is between 400 and 500 mm?**

```{r}
p_500_400 <- p_500 - p_400

p_500_400
```


### **Q4. (3 pts) What is the return period for a year with AT LEAST 550 mm of precip? The return period, Tr, is calculated as Tr = 1/p, with p being the probability for an event to occur.**

```{r}
p_550 <- pnorm(550, mean(climate_an$tot_p), sd(climate_an$tot_p))
ex_550 <- 1 - p_550

ri <- 1/ex_550
ri
```

Now lets think about what assumptions make return intervals valid. In the previous questions, you calculated probabilities and return intervals assuming that the historical climate record can be used to represent the future. These calculations rely on two key statistical assumptions:

The data follow a known probability distribution (often assumed to be normal).

The statistical properties of the data (mean, variance) do not change over time (stationarity).

In this question, you will evaluate whether those assumptions are reasonable for the annual precipitation record.

**Q5. (8 pts total)** 
**a. Explain why probability analysis of climate data assumes**
**- data are normally distributed**
**- and stationary?** 
  
**b. Using the code below, create a histogram of total annual precipitation (tot_p) with a density curve overlaid. Briefly comment on whether the distribution appears approximately normal (e.g., symmetry, skewness, tails).**

**c. Next, Use the Shapiro–Wilk test (code below) to formally test for normality.**

**State:**
**- the null hypothesis**
**- the p-value**
**-whether you reject normality at α = 0.05**

**d. Stationarity (2 pts)**
**Use the Ljung–Box test to assess whether the time series shows significant autocorrelation (i.e., non-stationarity).**
**Explain what the result implies about stationarity and the validity of return interval estimates.**

- [here](https://rpubs.com/richkt/269797){target="_blank"}  

- [here](https://www.statology.org/kpss-test-in-r/){target="_blank"} 

- [here](http://www.sthda.com/english/wiki/normality-test-in-r){target="_blank"}

```{r}
climate_an %>% 
  ggplot(aes(x = tot_p)) +
  theme_bw() +
  geom_histogram(binwidth = 20, aes(y = after_stat(density)), colour = "black", fill = "gray") + # histogram
  geom_density(alpha = 0.2, fill = "red") # density plot
```

```{r}
shapiro.test(climate_an$tot_p)

lag.length = 25
Box.test(climate_an$tot_p, lag = lag.length, type = "Ljung-Box") # test stationary signal
```




